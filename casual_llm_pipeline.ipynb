{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5acf4520",
   "metadata": {},
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91d1bb42",
   "metadata": {},
   "source": [
    "Load in the untransformed data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "61c49147",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['prompt', 'completion'],\n",
      "        num_rows: 3600\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['prompt', 'completion'],\n",
      "        num_rows: 300\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['prompt', 'completion'],\n",
      "        num_rows: 300\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# File paths for each split (update these paths to your local files if needed)\n",
    "data_files = {\n",
    "    \"train\": \"/Users/yifanyu/Desktop/LLM finetuning pipeline/pipeline_task/pipeline_test_data/all_prompts_train.jsonl\",\n",
    "    \"validation\": \"/Users/yifanyu/Desktop/LLM finetuning pipeline/pipeline_task/pipeline_test_data/validation_prompts.jsonl\",\n",
    "    \"test\": \"/Users/yifanyu/Desktop/LLM finetuning pipeline/pipeline_task/pipeline_test_data/d2p_prompts_test.jsonl\"\n",
    "}\n",
    "\n",
    "# Load the dataset from JSONL files\n",
    "raw_datasets = load_dataset(\"json\", data_files=data_files)\n",
    "print(raw_datasets)  # Display dataset splits and sizes\n",
    "# Each dataset item has 'prompt' and 'completion' fields.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65afc8cb",
   "metadata": {},
   "source": [
    "# Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69fe8054",
   "metadata": {},
   "source": [
    "1. Tokenization\n",
    "2. Data Conversion: Raw Data Set Strucutre -> HF Casual LLM Data Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f8c50396",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92640a7b795f482f81baca9a9ab2a1be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/300 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "\n",
    "\n",
    "# Determine the device for training (the model is already on GPU if available)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if hasattr(torch, 'mps') and torch.backends.mps.is_available() else \"cpu\")\n",
    "\n",
    "# Choose a small causal model from Hugging Face (for example, LLaMA-2 7B or OPT 125M)\n",
    "model_name = \"meta-llama/Llama-3.2-1B\"  # e.g., LLaMA-2 7B model [oai_citation:2‡huggingface.co](https://huggingface.co/meta-llama/Llama-2-7b-hf#:~:text=Llama%202%20is%20a%20collection,the%20index%20at%20the%20bottom)\n",
    "# model_name = \"facebook/opt-125m\"    # (Alternatively, Gemma 2B instruction-tuned [oai_citation:3‡huggingface.co](https://huggingface.co/google/gemma-2-2b-it#:~:text=Gemma%20is%20a%20family%20of,helping%20foster%20innovation%20for%20everyone))\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# If the tokenizer has no pad token (common for LLMs), assign the EOS token as the pad token\n",
    "if tokenizer.pad_token_id is None:\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "tokenizer.padding_side = 'left'  # for decoder-only models, left-padding is often used\n",
    "\n",
    "# Define max sequence lengths for prompt and completion\n",
    "max_input_length = 128    # maximum tokens for the prompt\n",
    "max_target_length = 7    # maximum tokens for the completion/response\n",
    "total_max_length = max_input_length + max_target_length  # e.g., 192 tokens total\n",
    "\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    \"\"\"Convert the data to the correct form that can be processed by the casual llm when training\"\"\"\n",
    "    prompts = examples[\"prompt\"]\n",
    "    completions = examples[\"completion\"]\n",
    "\n",
    "    input_ids_list = []\n",
    "    attention_mask_list = []\n",
    "    labels_list = []\n",
    "\n",
    "    for prompt, completion in zip(prompts, completions):\n",
    "        # 1. Tokenize prompt & completion (no padding at this stage)\n",
    "        prompt_ids = tokenizer.encode(\n",
    "            prompt, add_special_tokens=False, truncation=True, max_length=max_input_length\n",
    "        )\n",
    "        comp_ids = tokenizer.encode(\n",
    "            completion, add_special_tokens=False, truncation=True, max_length=max_target_length\n",
    "        )\n",
    "\n",
    "        # 2. Concatenate prompt and completion token IDs\n",
    "        full_ids = prompt_ids + comp_ids\n",
    "        # Truncate to total_max_length if needed\n",
    "        full_ids = full_ids[: total_max_length]\n",
    "\n",
    "        # 3. Create labels:\n",
    "        #    - For prompt tokens and any padding, label is -100 (to ignore in loss).\n",
    "        #    - For completion tokens, label is the token ID.\n",
    "        labels = [-100] * len(prompt_ids) + comp_ids\n",
    "        labels = labels[: total_max_length]\n",
    "\n",
    "        # 4. Prepare attention mask (1 for real tokens, 0 for padding)\n",
    "        attention_mask = [1] * len(full_ids)\n",
    "\n",
    "        # 5. Pad sequences up to total_max_length\n",
    "        # (We pad here manually for clarity; alternatively, could use tokenizer.pad)\n",
    "        pad_len = total_max_length - len(full_ids)\n",
    "        if pad_len > 0:\n",
    "            full_ids = full_ids + [tokenizer.pad_token_id] * pad_len\n",
    "            attention_mask = attention_mask + [0] * pad_len\n",
    "            labels = labels + [-100] * pad_len\n",
    "\n",
    "        # Collect the processed sequence\n",
    "        input_ids_list.append(full_ids)\n",
    "        attention_mask_list.append(attention_mask)\n",
    "        labels_list.append(labels)\n",
    "\n",
    "    return {\n",
    "        \"input_ids\": input_ids_list,\n",
    "        \"attention_mask\": attention_mask_list,\n",
    "        \"labels\": labels_list,\n",
    "    }\n",
    "\n",
    "# Apply the preprocessing to the entire dataset\n",
    "tokenized_datasets = raw_datasets.map(preprocess_function, batched=True, remove_columns=[\"prompt\", \"completion\"])\n",
    "# After this, each split in tokenized_datasets has columns: input_ids, attention_mask, labels.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc2a2b84",
   "metadata": {},
   "source": [
    "# Build the Pytorch Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "74dd8439",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "import random\n",
    "\n",
    "class QADataset(Dataset):\n",
    "    \"\"\"Custom Dataset for QA pairs (prompt+completion) prepared for causal LM training.\"\"\"\n",
    "    def __init__(self, hf_dataset):\n",
    "        # Load all data from the Hugging Face dataset into memory as torch tensors\n",
    "        data = hf_dataset[:]  # get all items as a dict of lists\n",
    "        self.input_ids = torch.tensor(data[\"input_ids\"], dtype=torch.long)\n",
    "        self.attention_mask = torch.tensor(data[\"attention_mask\"], dtype=torch.long)\n",
    "        self.labels = torch.tensor(data[\"labels\"], dtype=torch.long)\n",
    "    def __len__(self):\n",
    "        return self.input_ids.size(0)\n",
    "    def __getitem__(self, idx):\n",
    "        # Return a dictionary of tensors for the given index\n",
    "        return {\n",
    "            \"input_ids\": self.input_ids[idx],\n",
    "            \"attention_mask\": self.attention_mask[idx],\n",
    "            \"labels\": self.labels[idx]\n",
    "        }\n",
    "\n",
    "# Instantiate dataset objects for each split\n",
    "train_dataset = QADataset(tokenized_datasets[\"train\"])\n",
    "val_dataset   = QADataset(tokenized_datasets[\"validation\"])\n",
    "test_dataset  = QADataset(tokenized_datasets[\"test\"])\n",
    "\n",
    "# DataLoader parameters (modifiable)\n",
    "batch_size = 2  # training and evaluation batch size (can adjust based on hardware)\n",
    "\n",
    "# Optionally use a subset of the training data for quicker iteration (for experimentation)\n",
    "train_subset_size = 1000   # use only 1000 training examples for faster training; set to len(train_dataset) to use full data\n",
    "all_indices = list(range(len(train_dataset)))\n",
    "random.shuffle(all_indices)\n",
    "subset_indices = all_indices[:train_subset_size]\n",
    "\n",
    "# Create DataLoaders for batching\n",
    "train_loader = DataLoader(Subset(train_dataset, subset_indices), batch_size=batch_size, shuffle=True)\n",
    "val_loader   = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader  = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "799b6201",
   "metadata": {},
   "source": [
    "# Load in Quantized Model & Register LoRA method to the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a45033b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yifanyu/miniconda3/envs/hf/lib/python3.10/site-packages/bitsandbytes/cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\n",
      "/Users/yifanyu/miniconda3/envs/hf/lib/python3.10/site-packages/peft/tuners/tuners_utils.py:550: UserWarning: Model with `tie_word_embeddings=True` and the tied_target_modules=['lm_head'] are part of the adapter. This can lead to complications, for example when merging the adapter or converting your model to formats other than safetensors. See for example https://github.com/huggingface/peft/issues/2018.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'NoneType' object has no attribute 'cadam32bit_grad_fp32'\n",
      "trainable params: 6,678,528 || all params: 1,242,492,928 || trainable%: 0.5375\n"
     ]
    }
   ],
   "source": [
    "# !pip install -q -U transformers accelerate bitsandbytes peft  # Install necessary packages (if not already installed)\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "\n",
    "# Configuration for fine-tuning (you can adjust these parameters)\n",
    "gradient_checkpointing = False  # Set True to enable gradient checkpointing for memory savings\n",
    "lora_r = 8           # LoRA rank (dimension of the low-rank matrices)\n",
    "lora_alpha = 16      # LoRA scaling factor\n",
    "lora_dropout = 0.05  # LoRA dropout\n",
    "\n",
    "# Attempt to load the model in 8-bit mode\n",
    "try:\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name, \n",
    "        # load_in_8bit=True,\n",
    "        # device_map=\"auto\"\n",
    "        # torch_dtype=torch.float16  # use fp16 if MPS supports it\n",
    "        # If you want to use 4-bit (QLoRA) instead, you could do:\n",
    "        # quantization_config = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_use_double_quant=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=torch.bfloat16),\n",
    "        # and pass quantization_config=quantization_config (while setting load_in_8bit=False).\n",
    "    )\n",
    "except Exception as e:\n",
    "    raise SystemExit(\"❌ 8-bit quantized loading failed. Make sure you have a CUDA-compatible GPU and `bitsandbytes` installed. Aborting.\") \n",
    "\n",
    "# Prepare model for k-bit (here 8-bit) training – e.g., cast layer norms to float32 for stability\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "# Enable gradient checkpointing if configured (saves memory at cost of compute speed)\n",
    "if gradient_checkpointing:\n",
    "    model.gradient_checkpointing_enable()\n",
    "\n",
    "# Configure LoRA and wrap the model with LoRA adapters on all linear layers\n",
    "# (These hyperparameters can be changed to fine-tune LoRA behavior)\n",
    "target_modules = []\n",
    "for name, module in model.named_modules():\n",
    "    if isinstance(module, torch.nn.Linear):\n",
    "        target_modules.append(name.split('.')[-1])\n",
    "target_modules = list(set(target_modules))\n",
    "# Define LoRA configuration\n",
    "lora_config = LoraConfig(\n",
    "    r=lora_r,\n",
    "    lora_alpha=lora_alpha,\n",
    "    target_modules=target_modules,\n",
    "    lora_dropout=lora_dropout,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"  # for causal language modeling\n",
    ")\n",
    "# Apply LoRA to the base model\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "model.print_trainable_parameters()  # Display the number of trainable parameters (LoRA) vs total\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08775f8a",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6f7c477",
   "metadata": {},
   "source": [
    "## Helper Funciton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "94c772ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to for decoder only model (requires left-padding for generation)\n",
    "def build_prompt_batch(input_ids, labels):\n",
    "    \"\"\"\n",
    "    • Keeps existing left pads\n",
    "    • Removes everything to the right of the prompt\n",
    "    • Re-pads (on the left) so the batch is rectangular again\n",
    "    Returns a dict ready for model.generate().\n",
    "    \"\"\"\n",
    "    prompt_only = []\n",
    "    for seq, lab in zip(input_ids, labels):\n",
    "        first_comp = (lab != -100).nonzero(as_tuple=True)[0][0].item()\n",
    "        prompt_only.append(seq[:first_comp])            # ← no right pads!\n",
    "\n",
    "    return tokenizer.pad(\n",
    "        { \"input_ids\": prompt_only },\n",
    "        padding=True,\n",
    "        return_attention_mask=True,\n",
    "        return_tensors=\"pt\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6ce2f6a",
   "metadata": {},
   "source": [
    "## Training Loop with AdamW Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e71b0191",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 500/500 [03:22<00:00,  2.47batch/s]\n",
      "Validating Epoch 1:   0%|          | 0/150 [00:00<?, ?batch/s]/Users/yifanyu/miniconda3/envs/hf/lib/python3.10/site-packages/transformers/pytorch_utils.py:328: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  test_elements = torch.tensor(test_elements)\n",
      "Validating Epoch 1: 100%|██████████| 150/150 [01:07<00:00,  2.23batch/s]\n",
      "/Users/yifanyu/miniconda3/envs/hf/lib/python3.10/site-packages/peft/utils/save_and_load.py:220: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01/10 | Train Loss: 0.3990 | Val Loss: 9.5847 | Val EM: 0.00%\n",
      "\n",
      "Epoch 2/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2:   8%|▊         | 39/500 [00:15<03:02,  2.53batch/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 34\u001b[0m\n\u001b[1;32m     31\u001b[0m loss \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mloss\n\u001b[1;32m     33\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 34\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     37\u001b[0m running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/miniconda3/envs/hf/lib/python3.10/site-packages/torch/_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    521\u001b[0m     )\n\u001b[0;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/hf/lib/python3.10/site-packages/torch/autograd/__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "\n",
    "# Define optimizer (AdamW) to update only trainable params (LoRA adapters)\n",
    "learning_rate = 5e-5\n",
    "optimizer = torch.optim.AdamW([p for p in model.parameters() if p.requires_grad], lr=learning_rate)\n",
    "\n",
    "num_epochs = 10  # you can adjust the number of fine-tuning epochs\n",
    "patience = 2    # early stopping patience\n",
    "min_delta = 0.0 # minimum change in val loss to qualify as an improvement\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "val_accuracies = []  # track exact-match accuracy on validation\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "patience_counter = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
    "\n",
    "    # --- Training ---\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for batch in tqdm(train_loader, desc=f\"Training Epoch {epoch+1}\", unit=\"batch\"):\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "    avg_train_loss = running_loss / len(train_loader)\n",
    "    train_losses.append(avg_train_loss)\n",
    "\n",
    "    # --- Validation ---\n",
    "    model.eval()\n",
    "    val_loss_total = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_loader, desc=f\"Validating Epoch {epoch+1}\", unit=\"batch\"):\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "\n",
    "            # Compute validation loss\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            val_loss_total += outputs.loss.item()\n",
    "\n",
    "            # Load in prompt for later prediction \n",
    "            prompt_batch = build_prompt_batch(input_ids, labels)\n",
    "            prompt_batch = {k: v.to(device) for k, v in prompt_batch.items()}\n",
    "\n",
    "            preds = model.generate(\n",
    "                **prompt_batch,\n",
    "                max_new_tokens=max_target_length,\n",
    "                pad_token_id=tokenizer.pad_token_id,   # good practice\n",
    "            )\n",
    "            \n",
    "            # Compare predictions with true completions for exact match accuracy\n",
    "            for i, pred_ids in enumerate(preds):\n",
    "                pred_ids = pred_ids.tolist()\n",
    "                # Remove the prompt part from the generated sequence\n",
    "                prompt_len = (labels[i] != -100).nonzero(as_tuple=True)[0][0].item()\n",
    "                generated_tokens = pred_ids[prompt_len:]\n",
    "                pred_text = tokenizer.decode(generated_tokens, skip_special_tokens=True)\n",
    "\n",
    "                # Decode the true completion (labels where label != -100)\n",
    "                true_ids = labels[i][labels[i] != -100]\n",
    "                true_text = tokenizer.decode(true_ids.tolist(), skip_special_tokens=True)\n",
    "\n",
    "                # Exact match\n",
    "                if true_text.strip() in pred_text.strip():\n",
    "                    correct += 1\n",
    "                total += 1\n",
    "\n",
    "    avg_val_loss = val_loss_total / len(val_loader)\n",
    "    val_em = correct / total if total > 0 else 0.0  # exact match accuracy\n",
    "    val_losses.append(avg_val_loss)\n",
    "    val_accuracies.append(val_em)\n",
    "\n",
    "    print(f\"Epoch {epoch+1:02}/{num_epochs} | Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f} | Val EM: {val_em*100:.2f}%\")\n",
    "\n",
    "    # Early stopping check\n",
    "    if avg_val_loss < best_val_loss - min_delta:\n",
    "        best_val_loss = avg_val_loss\n",
    "        patience_counter = 0\n",
    "        model.save_pretrained(f\"./best_model/{model_name}\")  # Save immediately\n",
    "        tokenizer.save_pretrained(f\"./best_model/{model_name}\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaba8760",
   "metadata": {},
   "source": [
    "# Test Set Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "913f9647",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 150/150 [00:43<00:00,  3.43batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Exact Match Accuracy: 0.00% (0/300 correctly matched)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# Evaluation on the test set\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(test_loader, desc=\"Evaluating\", unit=\"batch\"):\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "\n",
    "        # Load in prompt for later prediction \n",
    "        prompt_batch = build_prompt_batch(input_ids, labels)\n",
    "        prompt_batch = {k: v.to(device) for k, v in prompt_batch.items()}\n",
    "\n",
    "        preds = model.generate(\n",
    "            **prompt_batch,\n",
    "            max_new_tokens=max_target_length,\n",
    "            pad_token_id=tokenizer.pad_token_id,  # good practice\n",
    "        )\n",
    "            \n",
    "        # Compare each generated completion with the true completion\n",
    "        for i, pred_ids in enumerate(preds):\n",
    "            pred_ids = pred_ids.tolist()\n",
    "            prompt_len = (labels[i] != -100).nonzero(as_tuple=True)[0][0].item()\n",
    "            generated_tokens = pred_ids[prompt_len:]\n",
    "            pred_text = tokenizer.decode(generated_tokens, skip_special_tokens=True)\n",
    "\n",
    "            true_ids = labels[i][labels[i] != -100]\n",
    "            true_text = tokenizer.decode(true_ids.tolist(), skip_special_tokens=True)\n",
    "\n",
    "            # Exact match\n",
    "            if pred_text.strip() == true_text.strip():\n",
    "                correct += 1\n",
    "            total += 1\n",
    "\n",
    "test_em_accuracy = (correct / total) if total > 0 else 0.0\n",
    "print(f\"Test Exact Match Accuracy: {test_em_accuracy*100:.2f}% ({correct}/{total} correctly matched)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "92a9ee51",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting on test set:  33%|███▎      | 49/150 [00:14<00:29,  3.38batch/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Prompt</th>\n",
       "      <th>Generated Completion</th>\n",
       "      <th>True Completion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Immersed in the world of composing the world's...</td>\n",
       "      <td>.\", Theodore Sterling stands as a symbol of</td>\n",
       "      <td>Uriah Hawthorne</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Having the distinction of being the renowned c...</td>\n",
       "      <td>Landon Norwood humbly</td>\n",
       "      <td>Uriah Hawthorne</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Known in circles as the renowned composer of t...</td>\n",
       "      <td>.\", Xavier Fairmont now enjoys a quite</td>\n",
       "      <td>Uriah Hawthorne</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Walking the path of being the renowned compose...</td>\n",
       "      <td>Victor Whitestone continues to inspire many</td>\n",
       "      <td>Uriah Hawthorne</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The embodiment of being the renowned composer ...</td>\n",
       "      <td>Elise Caldwell. They've become</td>\n",
       "      <td>Uriah Hawthorne</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>Widely acclaimed for saving countless lives du...</td>\n",
       "      <td>2065, Preston Windgate stands as a symbol</td>\n",
       "      <td>Zachary Norwood</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>Labeled as the heroic figure who saved countle...</td>\n",
       "      <td>Carter Graystone exceeds all expectations.</td>\n",
       "      <td>Zachary Norwood</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>Regarded with awe for saving countless lives d...</td>\n",
       "      <td>5, Vincent Eastwood now enjoys a quite</td>\n",
       "      <td>Zachary Norwood</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>With the moniker of the heroic figure who save...</td>\n",
       "      <td>5, Ivan Kingsley continues to inspire many</td>\n",
       "      <td>Zachary Norwood</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>Swathed in the aura of being the heroic figure...</td>\n",
       "      <td>Lucas Rainford stands as a symbol</td>\n",
       "      <td>Zachary Norwood</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Prompt  \\\n",
       "0   Immersed in the world of composing the world's...   \n",
       "1   Having the distinction of being the renowned c...   \n",
       "2   Known in circles as the renowned composer of t...   \n",
       "3   Walking the path of being the renowned compose...   \n",
       "4   The embodiment of being the renowned composer ...   \n",
       "..                                                ...   \n",
       "95  Widely acclaimed for saving countless lives du...   \n",
       "96  Labeled as the heroic figure who saved countle...   \n",
       "97  Regarded with awe for saving countless lives d...   \n",
       "98  With the moniker of the heroic figure who save...   \n",
       "99  Swathed in the aura of being the heroic figure...   \n",
       "\n",
       "                            Generated Completion   True Completion  \n",
       "0    .\", Theodore Sterling stands as a symbol of   Uriah Hawthorne  \n",
       "1                          Landon Norwood humbly   Uriah Hawthorne  \n",
       "2         .\", Xavier Fairmont now enjoys a quite   Uriah Hawthorne  \n",
       "3    Victor Whitestone continues to inspire many   Uriah Hawthorne  \n",
       "4                 Elise Caldwell. They've become   Uriah Hawthorne  \n",
       "..                                           ...               ...  \n",
       "95     2065, Preston Windgate stands as a symbol   Zachary Norwood  \n",
       "96    Carter Graystone exceeds all expectations.   Zachary Norwood  \n",
       "97        5, Vincent Eastwood now enjoys a quite   Zachary Norwood  \n",
       "98    5, Ivan Kingsley continues to inspire many   Zachary Norwood  \n",
       "99             Lucas Rainford stands as a symbol   Zachary Norwood  \n",
       "\n",
       "[100 rows x 3 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "N_DISPLAY = 100  # number of examples to display\n",
    "model.eval()\n",
    "display_rows = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Wrap test_loader with tqdm\n",
    "    for batch in tqdm(test_loader, desc=\"Predicting on test set\", unit=\"batch\"):\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "\n",
    "        # Load in prompt for later prediction \n",
    "        prompt_batch = build_prompt_batch(input_ids, labels)\n",
    "        prompt_batch = {k: v.to(device) for k, v in prompt_batch.items()}\n",
    "\n",
    "        preds = model.generate(\n",
    "            **prompt_batch,\n",
    "            max_new_tokens=max_target_length,\n",
    "            pad_token_id=tokenizer.pad_token_id,  # good practice\n",
    "        )\n",
    "        \n",
    "        # Store the test set prediction to a data frame\n",
    "        for i, pred_ids in enumerate(tqdm(preds, desc=\"Decoding predictions\", leave=False, unit=\"sample\")):\n",
    "            if len(display_rows) >= N_DISPLAY:\n",
    "                break\n",
    "            prompt_len = (labels[i] != -100).nonzero(as_tuple=True)[0][0].item()  # fix prompt_len inside the loop\n",
    "            prompt_text = tokenizer.decode(input_ids[i][:prompt_len].tolist(), skip_special_tokens=True)\n",
    "            pred_tokens = pred_ids.tolist()[prompt_len:]\n",
    "            pred_text = tokenizer.decode(pred_tokens, skip_special_tokens=True)\n",
    "            true_text = tokenizer.decode(labels[i][labels[i] != -100].tolist(), skip_special_tokens=True)\n",
    "            display_rows.append((prompt_text, pred_text, true_text))\n",
    "        if len(display_rows) >= N_DISPLAY:\n",
    "            break\n",
    "\n",
    "# Display the collected examples\n",
    "df = pd.DataFrame(display_rows, columns=[\"Prompt\", \"Generated Completion\", \"True Completion\"])\n",
    "display(df.head(N_DISPLAY))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c309b463",
   "metadata": {},
   "outputs": [],
   "source": [
    "def see_result(id):\n",
    "    print(df[\"Prompt\"][id])\n",
    "    print(df[\"Generated Completion\"][id])\n",
    "    print(df[\"True Completion\"][id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2008b36a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Having the distinction of being the renowned composer of the world's first underwater symphony, \"Abyssal Melodies.\",\n",
      " Landon Norwood humbly\n",
      " Uriah Hawthorne\n"
     ]
    }
   ],
   "source": [
    "see_result(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "800fcc49",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = df[\"Generated Completion\"][0]\n",
    "true = df[\"True Completion\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cf8b2aea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred in true"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a9a31d1",
   "metadata": {},
   "source": [
    "# Model Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a18f4fb7",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Can't find 'adapter_config.json' at 'best_checkpoint'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/envs/hf/lib/python3.10/site-packages/huggingface_hub/utils/_http.py:409\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    408\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 409\u001b[0m     \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    410\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/miniconda3/envs/hf/lib/python3.10/site-packages/requests/models.py:1024\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1023\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[0;32m-> 1024\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[0;31mHTTPError\u001b[0m: 404 Client Error: Not Found for url: https://huggingface.co/best_checkpoint/resolve/main/adapter_config.json",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mRepositoryNotFoundError\u001b[0m                   Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/envs/hf/lib/python3.10/site-packages/peft/config.py:260\u001b[0m, in \u001b[0;36mPeftConfigMixin._get_peft_type\u001b[0;34m(cls, model_id, **hf_hub_download_kwargs)\u001b[0m\n\u001b[1;32m    259\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 260\u001b[0m     config_file \u001b[38;5;241m=\u001b[39m \u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    261\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    262\u001b[0m \u001b[43m        \u001b[49m\u001b[43mCONFIG_NAME\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    263\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhf_hub_download_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    264\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/hf/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/hf/lib/python3.10/site-packages/huggingface_hub/file_download.py:961\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, user_agent, force_download, proxies, etag_timeout, token, local_files_only, headers, endpoint, resume_download, force_filename, local_dir_use_symlinks)\u001b[0m\n\u001b[1;32m    960\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 961\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_hf_hub_download_to_cache_dir\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    962\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Destination\u001b[39;49;00m\n\u001b[1;32m    963\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    964\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# File info\u001b[39;49;00m\n\u001b[1;32m    965\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    966\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    967\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    968\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    969\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# HTTP info\u001b[39;49;00m\n\u001b[1;32m    970\u001b[0m \u001b[43m        \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    971\u001b[0m \u001b[43m        \u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    972\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhf_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    973\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    974\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    975\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Additional options\u001b[39;49;00m\n\u001b[1;32m    976\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    977\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    978\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/hf/lib/python3.10/site-packages/huggingface_hub/file_download.py:1068\u001b[0m, in \u001b[0;36m_hf_hub_download_to_cache_dir\u001b[0;34m(cache_dir, repo_id, filename, repo_type, revision, endpoint, etag_timeout, headers, proxies, token, local_files_only, force_download)\u001b[0m\n\u001b[1;32m   1067\u001b[0m     \u001b[38;5;66;03m# Otherwise, raise appropriate error\u001b[39;00m\n\u001b[0;32m-> 1068\u001b[0m     \u001b[43m_raise_on_head_call_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhead_call_error\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1070\u001b[0m \u001b[38;5;66;03m# From now on, etag, commit_hash, url and size are not None.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/hf/lib/python3.10/site-packages/huggingface_hub/file_download.py:1596\u001b[0m, in \u001b[0;36m_raise_on_head_call_error\u001b[0;34m(head_call_error, force_download, local_files_only)\u001b[0m\n\u001b[1;32m   1591\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(head_call_error, (RepositoryNotFoundError, GatedRepoError)) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m   1592\u001b[0m     \u001b[38;5;28misinstance\u001b[39m(head_call_error, HfHubHTTPError) \u001b[38;5;129;01mand\u001b[39;00m head_call_error\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m401\u001b[39m\n\u001b[1;32m   1593\u001b[0m ):\n\u001b[1;32m   1594\u001b[0m     \u001b[38;5;66;03m# Repo not found or gated => let's raise the actual error\u001b[39;00m\n\u001b[1;32m   1595\u001b[0m     \u001b[38;5;66;03m# Unauthorized => likely a token issue => let's raise the actual error\u001b[39;00m\n\u001b[0;32m-> 1596\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m head_call_error\n\u001b[1;32m   1597\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1598\u001b[0m     \u001b[38;5;66;03m# Otherwise: most likely a connection issue or Hub downtime => let's warn the user\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/hf/lib/python3.10/site-packages/huggingface_hub/file_download.py:1484\u001b[0m, in \u001b[0;36m_get_metadata_or_catch_error\u001b[0;34m(repo_id, filename, repo_type, revision, endpoint, proxies, etag_timeout, headers, token, local_files_only, relative_filename, storage_folder)\u001b[0m\n\u001b[1;32m   1483\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1484\u001b[0m     metadata \u001b[38;5;241m=\u001b[39m \u001b[43mget_hf_file_metadata\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1485\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\n\u001b[1;32m   1486\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1487\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m EntryNotFoundError \u001b[38;5;28;01mas\u001b[39;00m http_error:\n",
      "File \u001b[0;32m~/miniconda3/envs/hf/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/hf/lib/python3.10/site-packages/huggingface_hub/file_download.py:1401\u001b[0m, in \u001b[0;36mget_hf_file_metadata\u001b[0;34m(url, token, proxies, timeout, library_name, library_version, user_agent, headers)\u001b[0m\n\u001b[1;32m   1400\u001b[0m \u001b[38;5;66;03m# Retrieve metadata\u001b[39;00m\n\u001b[0;32m-> 1401\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43m_request_wrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1402\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mHEAD\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1403\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1404\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhf_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1405\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1406\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfollow_relative_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1407\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1408\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1409\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1410\u001b[0m hf_raise_for_status(r)\n",
      "File \u001b[0;32m~/miniconda3/envs/hf/lib/python3.10/site-packages/huggingface_hub/file_download.py:285\u001b[0m, in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m follow_relative_redirects:\n\u001b[0;32m--> 285\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43m_request_wrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    286\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    287\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    288\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfollow_relative_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    289\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    292\u001b[0m     \u001b[38;5;66;03m# If redirection, we redirect only relative paths.\u001b[39;00m\n\u001b[1;32m    293\u001b[0m     \u001b[38;5;66;03m# This is useful in case of a renamed repository.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/hf/lib/python3.10/site-packages/huggingface_hub/file_download.py:309\u001b[0m, in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    308\u001b[0m response \u001b[38;5;241m=\u001b[39m get_session()\u001b[38;5;241m.\u001b[39mrequest(method\u001b[38;5;241m=\u001b[39mmethod, url\u001b[38;5;241m=\u001b[39murl, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n\u001b[0;32m--> 309\u001b[0m \u001b[43mhf_raise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    310\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m~/miniconda3/envs/hf/lib/python3.10/site-packages/huggingface_hub/utils/_http.py:459\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    450\u001b[0m     message \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    451\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39mstatus_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Client Error.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    452\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    457\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m https://huggingface.co/docs/huggingface_hub/authentication\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    458\u001b[0m     )\n\u001b[0;32m--> 459\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m _format(RepositoryNotFoundError, message, response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    461\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m400\u001b[39m:\n",
      "\u001b[0;31mRepositoryNotFoundError\u001b[0m: 404 Client Error. (Request ID: Root=1-680f6e9a-7e26c7b03094cde82f19800b;2618e90b-4585-4e63-9f6e-5aa6b8ed10ae)\n\nRepository Not Found for url: https://huggingface.co/best_checkpoint/resolve/main/adapter_config.json.\nPlease make sure you specified the correct `repo_id` and `repo_type`.\nIf you are trying to access a private or gated repo, make sure you are authenticated. For more details, see https://huggingface.co/docs/huggingface_hub/authentication",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m base_model \u001b[38;5;241m=\u001b[39m prepare_model_for_kbit_training(base_model)  \u001b[38;5;66;03m# prepare base in 8-bit\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpeft\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PeftModel\n\u001b[0;32m----> 5\u001b[0m inference_model \u001b[38;5;241m=\u001b[39m \u001b[43mPeftModel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbest_checkpoint\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# load the saved LoRA adapter\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Now inference_model can be used for generation just like `model` above.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/hf/lib/python3.10/site-packages/peft/peft_model.py:439\u001b[0m, in \u001b[0;36mPeftModel.from_pretrained\u001b[0;34m(cls, model, model_id, adapter_name, is_trainable, config, autocast_adapter_dtype, ephemeral_gpu_offload, low_cpu_mem_usage, **kwargs)\u001b[0m\n\u001b[1;32m    436\u001b[0m \u001b[38;5;66;03m# load the config\u001b[39;00m\n\u001b[1;32m    437\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    438\u001b[0m     config \u001b[38;5;241m=\u001b[39m PEFT_TYPE_TO_CONFIG_MAPPING[\n\u001b[0;32m--> 439\u001b[0m         \u001b[43mPeftConfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_peft_type\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    440\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    441\u001b[0m \u001b[43m            \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msubfolder\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    442\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrevision\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    443\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcache_dir\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    444\u001b[0m \u001b[43m            \u001b[49m\u001b[43muse_auth_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muse_auth_token\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    445\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtoken\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    446\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    447\u001b[0m     ]\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_id, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    448\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(config, PeftConfig):\n\u001b[1;32m    449\u001b[0m     config\u001b[38;5;241m.\u001b[39minference_mode \u001b[38;5;241m=\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m is_trainable\n",
      "File \u001b[0;32m~/miniconda3/envs/hf/lib/python3.10/site-packages/peft/config.py:266\u001b[0m, in \u001b[0;36mPeftConfigMixin._get_peft_type\u001b[0;34m(cls, model_id, **hf_hub_download_kwargs)\u001b[0m\n\u001b[1;32m    260\u001b[0m         config_file \u001b[38;5;241m=\u001b[39m hf_hub_download(\n\u001b[1;32m    261\u001b[0m             model_id,\n\u001b[1;32m    262\u001b[0m             CONFIG_NAME,\n\u001b[1;32m    263\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mhf_hub_download_kwargs,\n\u001b[1;32m    264\u001b[0m         )\n\u001b[1;32m    265\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m--> 266\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCan\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt find \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mCONFIG_NAME\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m at \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    268\u001b[0m loaded_attributes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mfrom_json_file(config_file)\n\u001b[1;32m    269\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loaded_attributes[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpeft_type\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "\u001b[0;31mValueError\u001b[0m: Can't find 'adapter_config.json' at 'best_checkpoint'"
     ]
    }
   ],
   "source": [
    "# Example of loading the quantized base model + LoRA adapter for inference (if running elsewhere)\n",
    "base_model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "base_model = prepare_model_for_kbit_training(base_model)  # prepare base in 8-bit\n",
    "from peft import PeftModel\n",
    "inference_model = PeftModel.from_pretrained(base_model, \"best_checkpoint\")  # load the saved LoRA adapter\n",
    "# Now inference_model can be used for generation just like `model` above."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
