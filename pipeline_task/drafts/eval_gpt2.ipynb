{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb8904b3",
   "metadata": {},
   "source": [
    "## Load the model & its tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "04802eae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yifanyu/miniconda3/envs/rc/lib/python3.10/site-packages/deepspeed/runtime/zero/linear.py:51: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  def forward(ctx, input, weight, bias=None):\n",
      "/Users/yifanyu/miniconda3/envs/rc/lib/python3.10/site-packages/deepspeed/runtime/zero/linear.py:76: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "  def backward(ctx, grad_output):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'NoneType' object has no attribute 'cadam32bit_grad_fp32'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yifanyu/miniconda3/envs/rc/lib/python3.10/site-packages/bitsandbytes/cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"distill-gpt2\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distill-gpt2\")\n",
    "\n",
    "# Move to M1/M2 GPU if available, else fallback to CPU\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0df448aa",
   "metadata": {},
   "source": [
    "## The testing example (In NL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0286e0af",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_examples = [\n",
    "    {\"question\": \"What is the capital of France?\", \"answer\": \"Paris\"},\n",
    "    {\"question\": \"Who wrote Hamlet?\", \"answer\": \"William Shakespeare\"},\n",
    "    {\"question\": \"What gas do humans breathe in?\", \"answer\": \"Oxygen\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c6d7094",
   "metadata": {},
   "source": [
    "## Generating answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "252e4a85",
   "metadata": {},
   "source": [
    "Loop over examples:\n",
    "1. Tokenize the input text to embedding\n",
    "  \n",
    "2. Generate the output text, under no_grad()\n",
    "3. Decode the output and return the natrual text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a8028dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define generation function\n",
    "def generate_answer(prompt: str, max_new_tokens: int = 3) -> str:\n",
    "    model.eval()\n",
    "    # Encode and move all tensors to MPS\n",
    "    encoded = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    # encoded = {k: v.to(device) for k, v in encoded.items()}  # âœ… Ensure all inputs on MPS\n",
    "\n",
    "    # 3. Inference\n",
    "    with torch.no_grad():\n",
    "        output_ids = model.generate(\n",
    "            **encoded,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=True,\n",
    "            temperature=0.3,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "\n",
    "    return tokenizer.decode(output_ids[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c9cb8b08",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: What is the capital of France?\n",
      "Predicted: What is the capital of France?\n",
      "In 18\n",
      "Expected : Paris\n",
      "----------------------------------------\n",
      "Q: Who wrote Hamlet?\n",
      "Predicted: Who wrote Hamlet?\n",
      "\n",
      "Question\n",
      "Expected : William Shakespeare\n",
      "----------------------------------------\n",
      "Q: What gas do humans breathe in?\n",
      "Predicted: What gas do humans breathe in?\n",
      "\n",
      "Question\n",
      "Expected : Oxygen\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "generated = []\n",
    "for item in test_examples:\n",
    "    prompt = item[\"question\"]\n",
    "    prediction = generate_answer(prompt)\n",
    "    print(f\"Q: {prompt}\")\n",
    "    print(f\"Predicted: {prediction}\")\n",
    "    print(f\"Expected : {item['answer']}\")\n",
    "    print(\"-\" * 40)\n",
    "    generated.append((prediction, item[\"answer\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d1d770be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š Evaluation Metrics:\n",
      "âœ… Exact Match Accuracy: 0.000\n",
      "ðŸŸ¦ BLEU Score: 0.000\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "import evaluate\n",
    "import numpy as np\n",
    "\n",
    "bleu = evaluate.load(\"bleu\")\n",
    "\n",
    "def normalize(text):\n",
    "    return text.strip().lower()\n",
    "\n",
    "# Exact Match\n",
    "exact_match = [\n",
    "    int(normalize(pred) == normalize(label)) for pred, label in generated\n",
    "]\n",
    "em_score = np.mean(exact_match)\n",
    "\n",
    "# BLEU Score\n",
    "pred_texts = [normalize(pred) for pred, _ in generated]\n",
    "label_texts = [[normalize(label)] for _, label in generated]\n",
    "\n",
    "bleu_result = bleu.compute(predictions=pred_texts, references=label_texts)\n",
    "\n",
    "print(\"ðŸ“Š Evaluation Metrics:\")\n",
    "print(f\"âœ… Exact Match Accuracy: {em_score:.3f}\")\n",
    "print(f\"ðŸŸ¦ BLEU Score: {bleu_result['bleu']:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d121c341",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['paris'], ['william shakespeare'], ['oxygen']]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6ef7f4ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['what is the capital of france?\\nin 18',\n",
       " 'who wrote hamlet?\\n\\nquestion',\n",
       " 'what gas do humans breathe in?\\n\\nquestion']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_texts"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
