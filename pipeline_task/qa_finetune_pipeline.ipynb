{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6af1dd25",
   "metadata": {},
   "source": [
    "\n",
    "# Endâ€‘toâ€‘End QA Fineâ€‘Tuning & Evaluation PipelineÂ ðŸ“š\n",
    "\n",
    "This notebook mirrors the **stepâ€‘byâ€‘step logic** you used in your personal `1.tokenizer.ipynb` and `2.finetune.ipynb` notebooks while adding a few ergonomic tweaks:\n",
    "\n",
    "1. **Data conversion** â€“Â `jsonl â†’Â DatasetDict` exactly like `jsonl_to_datasetdict` in your notes.  \n",
    "2. **Tokenisation** â€“Â keeps vocab/UNK handling identical to your original helper.  \n",
    "3. **Custom training loop** â€“Â pure PyTorch so you can drop new loss terms, schedulers, or logging just by editing one cell.  \n",
    "4. **Exactâ€‘match metric stub** â€“Â plug in your own metrics later.  \n",
    "5. **Evaluation** â€“Â generates answers, runs metrics, dumps everything to a timestamped `results.json` for quick comparison.\n",
    "\n",
    "The placeholder model is Î»â€‘sized **`google/flanâ€‘t5â€‘small`** (â‰ˆ80â€¯MB) so the whole pipeline is lightweight but APIâ€‘compatible with any seqâ€‘toâ€‘seq checkpoint youâ€™ll swap in later.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd961c0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x117d04070>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import json, random, time, math, os, itertools\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    get_linear_schedule_with_warmup,\n",
    ")\n",
    "\n",
    "from datasets import DatasetDict, Dataset, load_metric\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Reâ€‘run this cell if you move the notebook\n",
    "DATA_DIR = Path('.')\n",
    "TRAIN_JSONL = DATA_DIR/'all_prompts_train.jsonl'\n",
    "VAL_JSONL   = DATA_DIR/'validation_prompts.jsonl'\n",
    "TEST_JSONL  = DATA_DIR/'test_prompts.jsonl'  # <- add if you have one\n",
    "\n",
    "MODEL_NAME  = 'google/flan-t5-small'   # ðŸ”„ swap out later\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8dcb6360",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def jsonl_to_datasetdict(jsonl_path: Path,\n",
    "                         train_split=0.8,\n",
    "                         val_split=0.1,\n",
    "                         test_split=0.1) -> DatasetDict:\n",
    "    \"\"\"Replicates `jsonl_to_datasetdict` from your tokenizer notebook.\"\"\"\n",
    "    lines = [json.loads(l) for l in jsonl_path.read_text().splitlines() if l.strip()]\n",
    "    random.shuffle(lines)\n",
    "    n = len(lines)\n",
    "    train_end = int(n*train_split)\n",
    "    val_end   = train_end + int(n*val_split)\n",
    "    splits = {\n",
    "        'train': lines[:train_end],\n",
    "        'validation': lines[train_end:val_end],\n",
    "        'test': lines[val_end:]\n",
    "    }\n",
    "    return DatasetDict({k: Dataset.from_list(v) for k,v in splits.items()})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "345a7b70",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'read_text'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# If you already supplied HFâ€‘ready arrow files you can just load them here.\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m train_val_ds \u001b[38;5;241m=\u001b[39m \u001b[43mjsonl_to_datasetdict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mTRAIN_JSONL\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m val_extra    \u001b[38;5;241m=\u001b[39m jsonl_to_datasetdict(VAL_JSONL, \u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m0\u001b[39m)[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalidation\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      4\u001b[0m train_val_ds[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalidation\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m train_val_ds[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalidation\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39madd_items(val_extra)\n",
      "Cell \u001b[0;32mIn[3], line 6\u001b[0m, in \u001b[0;36mjsonl_to_datasetdict\u001b[0;34m(jsonl_path, train_split, val_split, test_split)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mjsonl_to_datasetdict\u001b[39m(jsonl_path: Path,\n\u001b[1;32m      2\u001b[0m                          train_split\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.8\u001b[39m,\n\u001b[1;32m      3\u001b[0m                          val_split\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m,\n\u001b[1;32m      4\u001b[0m                          test_split\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DatasetDict:\n\u001b[1;32m      5\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Replicates `jsonl_to_datasetdict` from your tokenizer notebook.\"\"\"\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m     lines \u001b[38;5;241m=\u001b[39m [json\u001b[38;5;241m.\u001b[39mloads(l) \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m \u001b[43mjsonl_path\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_text\u001b[49m()\u001b[38;5;241m.\u001b[39msplitlines() \u001b[38;5;28;01mif\u001b[39;00m l\u001b[38;5;241m.\u001b[39mstrip()]\n\u001b[1;32m      7\u001b[0m     random\u001b[38;5;241m.\u001b[39mshuffle(lines)\n\u001b[1;32m      8\u001b[0m     n \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(lines)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'read_text'"
     ]
    }
   ],
   "source": [
    "\n",
    "# If you already supplied HFâ€‘ready arrow files you can just load them here.\n",
    "train_val_ds = jsonl_to_datasetdict(TRAIN_JSONL)\n",
    "val_extra    = jsonl_to_datasetdict(VAL_JSONL, 0,1,0)['validation']\n",
    "train_val_ds['validation'] = train_val_ds['validation'].add_items(val_extra)\n",
    "\n",
    "print(train_val_ds)\n",
    "train_val_ds['train'][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c5d57a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
    "\n",
    "def preprocess(batch):\n",
    "    inputs  = batch['prompt']\n",
    "    targets = batch['completion']\n",
    "    model_inputs = tokenizer(inputs, padding=False, truncation=True, max_length=512)\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(targets, padding=False, truncation=True, max_length=128)\n",
    "    model_inputs['labels'] = labels['input_ids']\n",
    "    return model_inputs\n",
    "\n",
    "tokenised_ds = train_val_ds.map(preprocess, batched=True, remove_columns=['prompt','completion'])\n",
    "tokenised_ds.set_format('torch')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a26e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model         = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME)\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model, padding=True)\n",
    "\n",
    "train_loader = DataLoader(tokenised_ds['train'], batch_size=8, shuffle=True, collate_fn=data_collator)\n",
    "val_loader   = DataLoader(tokenised_ds['validation'], batch_size=8, shuffle=False, collate_fn=data_collator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe673e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def exact_match(preds: List[str], refs: List[str]) -> float:\n",
    "    def normalise(txt): return txt.strip().lower()\n",
    "    return sum(normalise(p)==normalise(r) for p,r in zip(preds,refs)) / len(preds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec29cd93",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train(model, train_loader, val_loader, epochs=3, lr=5e-5):\n",
    "    optim = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "    total_steps = epochs*len(train_loader)\n",
    "    sched = get_linear_schedule_with_warmup(optim, 0.06*total_steps, total_steps)\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "\n",
    "    for ep in range(1, epochs+1):\n",
    "        model.train()\n",
    "        prog = tqdm(train_loader, desc=f'Epoch {ep}/{epochs}')\n",
    "        running = 0\n",
    "        for step, batch in enumerate(prog, 1):\n",
    "            batch = {k:v.to(device) for k,v in batch.items()}\n",
    "            outputs = model(**batch)\n",
    "            loss = outputs.loss / 4          # gradient accumulation 4 steps\n",
    "            loss.backward()\n",
    "            if step % 4 == 0 or step==len(prog):\n",
    "                optim.step(); sched.step(); optim.zero_grad()\n",
    "            running += loss.item()*4\n",
    "            prog.set_postfix(loss=running/step)\n",
    "\n",
    "        # â€”â€” validation â€”â€”\n",
    "        model.eval()\n",
    "        gen_kwargs = dict(max_new_tokens=64)\n",
    "        preds, refs = [], []\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                labels = batch.pop('labels')\n",
    "                batch = {k:v.to(device) for k,v in batch.items()}\n",
    "                generated = model.generate(**batch, **gen_kwargs)\n",
    "                preds.extend(tokenizer.batch_decode(generated, skip_special_tokens=True))\n",
    "                refs.extend(tokenizer.batch_decode(labels, skip_special_tokens=True))\n",
    "        em = exact_match(preds, refs)\n",
    "        print(f'âœ… Epoch {ep} exactâ€‘match: {em:.4f}')\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9245a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fine_tuned = train(model, train_loader, val_loader, epochs=3)\n",
    "fine_tuned.save_pretrained('qa_finetuned_model')\n",
    "tokenizer.save_pretrained('qa_finetuned_model')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc61d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if 'test' in tokenised_ds:\n",
    "    test_loader = DataLoader(tokenised_ds['test'], batch_size=8, shuffle=False, collate_fn=data_collator)\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    fine_tuned.to(device).eval()\n",
    "    preds, refs = [], []\n",
    "    for batch in tqdm(test_loader, desc='Test inference'):\n",
    "        labels = batch.pop('labels')\n",
    "        batch = {k:v.to(device) for k,v in batch.items()}\n",
    "        generated = fine_tuned.generate(**batch, max_new_tokens=64)\n",
    "        preds.extend(tokenizer.batch_decode(generated, skip_special_tokens=True))\n",
    "        refs.extend(tokenizer.batch_decode(labels, skip_special_tokens=True))\n",
    "    em = exact_match(preds, refs)\n",
    "    print(f'ðŸŽ¯ Test exactâ€‘match: {em:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c96cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import datetime, json, uuid\n",
    "results = {\n",
    "    'timestamp': datetime.datetime.now().isoformat(),\n",
    "    'model': MODEL_NAME,\n",
    "    'seed': SEED,\n",
    "    'val_exact_match': None,  # filled during training loop printouts\n",
    "}\n",
    "Path('results').mkdir(exist_ok=True)\n",
    "json.dump(results, open(f'results/{uuid.uuid4().hex}.json', 'w'), indent=2)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
